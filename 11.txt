# Weighted Log-Mean Risk
df['weighted_log_mean_risk'] = np.log1p(
    0.4 * df['hct_mer_mcc_baysian_risk'] +
    0.3 * df['hct_term_state_baysian_risk'] +
    0.2 * df['ucm_avs_resp_baysian_risk'] +
    0.1 * df['hct_term_country_code_baysian_risk']
)

# Non-linear Risk Amplification
df['nonlinear_risk_amplification'] = (
    (df['hct_mer_mcc_baysian_risk'] ** 1.5) +
    (df['hct_term_state_baysian_risk'] ** 1.2) -
    (df['ucm_avs_resp_baysian_risk'] * 0.8)
)

# Risk Interaction Gradient
df['risk_interaction_gradient'] = (
    np.gradient([
        df['hct_mer_mcc_baysian_risk'],
        df['hct_term_state_baysian_risk'],
        df['ucm_avs_resp_baysian_risk'],
        df['hct_term_country_code_baysian_risk']
    ], axis=0).sum(axis=0)
)

# Bayesian Risk Power Combination
df['bayesian_risk_power_comb'] = (
    df['hct_mer_mcc_baysian_risk'] ** 2 +
    df['hct_term_state_baysian_risk'] ** 1.5 +
    df['ucm_avs_resp_baysian_risk'] ** 1.8 +
    df['hct_term_country_code_baysian_risk'] ** 2
)

# Risk Proximity Divergence
df['risk_proximity_divergence'] = (
    np.abs(df['hct_mer_mcc_baysian_risk'] - df['hct_term_state_baysian_risk']) +
    np.abs(df['ucm_avs_resp_baysian_risk'] - df['hct_term_country_code_baysian_risk'])
)

# Asymmetric Risk Interaction
df['asymmetric_risk_interaction'] = (
    (df['hct_mer_mcc_baysian_risk'] + 1) / (df['ucm_avs_resp_baysian_risk'] + 1)
) * (
    (df['hct_term_state_baysian_risk'] + 1) / (df['hct_term_country_code_baysian_risk'] + 1)
)

# Cross-Dimensional Risk Ratio
df['cross_dimensional_risk_ratio'] = (
    df['hct_mer_mcc_baysian_risk'] /
    (df['hct_term_state_baysian_risk'] + df['ucm_avs_resp_baysian_risk'] + 1e-6)
)

# Risk-Weighted Anomaly Index
df['risk_weighted_anomaly_index'] = (
    df['hct_mer_mcc_baysian_risk'] * df['tran_util'] +
    df['hct_term_state_baysian_risk'] * df['foreign_ind']
) / (
    df['ucm_avs_resp_baysian_risk'] + df['hct_term_country_code_baysian_risk'] + 1e-6
)

# Risk Saturation Effect
df['risk_saturation_effect'] = np.tanh(
    df['hct_mer_mcc_baysian_risk'] + df['hct_term_state_baysian_risk'] +
    df['ucm_avs_resp_baysian_risk'] + df['hct_term_country_code_baysian_risk']
)

# Multi-risk Index
df['multi_risk_index'] = (
    (df['hct_mer_mcc_baysian_risk'] + df['hct_term_state_baysian_risk'])**2 +
    (df['ucm_avs_resp_baysian_risk'] + df['hct_term_country_code_baysian_risk'])**2
)

# Differential Risk Spread
df['differential_risk_spread'] = (
    df['hct_mer_mcc_baysian_risk'] - df['hct_term_country_code_baysian_risk']
) * (
    df['ucm_avs_resp_baysian_risk'] - df['hct_term_state_baysian_risk']
)

# Geometric Mean Risk
df['geo_mean_risk'] = np.power(
    df['hct_mer_mcc_baysian_risk'] *
    df['hct_term_state_baysian_risk'] *
    df['ucm_avs_resp_baysian_risk'] *
    df['hct_term_country_code_baysian_risk'], 0.25
)

# Inverse Risk Amplification
df['inverse_risk_amplification'] = 1 / (
    df['hct_mer_mcc_baysian_risk'] +
    df['hct_term_state_baysian_risk'] +
    df['ucm_avs_resp_baysian_risk'] +
    df['hct_term_country_code_baysian_risk'] + 1e-6
)

# Risk Signal Entropy
df['risk_signal_entropy'] = -(
    df['hct_mer_mcc_baysian_risk'] * np.log1p(df['hct_mer_mcc_baysian_risk']) +
    df['hct_term_state_baysian_risk'] * np.log1p(df['hct_term_state_baysian_risk']) +
    df['ucm_avs_resp_baysian_risk'] * np.log1p(df['ucm_avs_resp_baysian_risk']) +
    df['hct_term_country_code_baysian_risk'] * np.log1p(df['hct_term_country_code_baysian_risk'])
)

# Risk Sharpe Ratio Proxy
df['risk_sharpe_ratio_proxy'] = (
    df['hct_mer_mcc_baysian_risk'] -
    df['hct_term_country_code_baysian_risk']
) / (
    df['hct_term_state_baysian_risk'] + 1e-6
)

# Risk Factor Interaction Magnitude
df['risk_interaction_magnitude'] = np.sqrt(
    df['hct_mer_mcc_baysian_risk']**2 +
    df['hct_term_state_baysian_risk']**2 +
    df['ucm_avs_resp_baysian_risk']**2 +
    df['hct_term_country_code_baysian_risk']**2
)

# Weighted Exponential Risk
df['weighted_exp_risk'] = np.exp(
    0.4 * df['hct_mer_mcc_baysian_risk'] +
    0.3 * df['hct_term_state_baysian_risk'] +
    0.2 * df['ucm_avs_resp_baysian_risk'] +
    0.1 * df['hct_term_country_code_baysian_risk']
)




# Multi-dimensional risk convergence
df['multi_dim_risk_convergence'] = (
    df['hct_mer_mcc_baysian_risk'] +
    df['hct_term_state_baysian_risk'] +
    df['ucm_avs_resp_baysian_risk'] +
    df['hct_term_country_code_baysian_risk']
) / 4

# Interaction between merchant risk and AVS response
df['merchant_avs_risk_interaction'] = (
    df['hct_mer_mcc_baysian_risk'] * df['ucm_avs_resp_baysian_risk']
)

# Risk-weighted geographic-merchant interaction
df['geo_merchant_risk_interaction'] = (
    df['hct_mer_mcc_baysian_risk'] * df['hct_term_state_baysian_risk']
)

# Quadratic combination of all risk factors
df['quadratic_risk_interaction'] = (
    df['hct_mer_mcc_baysian_risk']**2 +
    df['hct_term_state_baysian_risk']**2 +
    df['ucm_avs_resp_baysian_risk']**2 +
    df['hct_term_country_code_baysian_risk']**2
)

# Weighted mismatch interaction
df['avs_cvv_risk_mismatch'] = (
    (df['ucm_avs_resp_baysian_risk'] + df['hct_term_country_code_baysian_risk']) *
    (df['avs_mismatch_ind'] + df['cvv2_mismatch_ind'])
)

# Combined country and state risk as one metric
df['geo_combined_risk'] = (
    df['hct_term_country_code_baysian_risk'] + df['hct_term_state_baysian_risk']
)

# Risk-weighted token recurrence behavior
df['token_recur_risk'] = (
    df['hct_mer_mcc_baysian_risk'] *
    (df['token_recur_bill_ind'] + df['recur_bill_ind'])
)

# Risk-adjusted transaction utility
df['risk_adj_tran_util'] = (
    df['hct_mer_mcc_baysian_risk'] * df['tran_util']
)

# Cross-product of merchant and state risk
df['merchant_state_risk_cross'] = (
    df['hct_mer_mcc_baysian_risk'] * df['hct_term_state_baysian_risk']
)

# Normalized risk spread
df['risk_spread_normalized'] = (
    (df['hct_mer_mcc_baysian_risk'] + df['hct_term_state_baysian_risk']) /
    (df['ucm_avs_resp_baysian_risk'] + df['hct_term_country_code_baysian_risk'] + 1e-6)
)

# Risk inversion feature
df['inverse_geo_risk'] = 1 / (df['hct_term_country_code_baysian_risk'] + 1e-6)

# Exponential risk amplification
df['exp_risk_amplification'] = np.exp(
    df['hct_mer_mcc_baysian_risk'] + df['ucm_avs_resp_baysian_risk']
)

# Logarithmic merchant risk
df['log_merchant_risk'] = np.log1p(df['hct_mer_mcc_baysian_risk'])

# Interaction of risk factors with even-dollar transactions
df['even_dollar_risk_interaction'] = (
    df['hct_term_country_code_baysian_risk'] * df['even_dollar_ind']
)

# Square root risk transformation
df['sqrt_combined_risk'] = np.sqrt(
    df['hct_mer_mcc_baysian_risk'] + df['hct_term_country_code_baysian_risk']
)

# Risk-mismatch divergence
df['risk_mismatch_divergence'] = np.abs(
    df['hct_term_state_baysian_risk'] - df['ucm_avs_resp_baysian_risk']
)

# Risk harmonization feature
df['harmonic_mean_risk'] = 4 / (
    1/(df['hct_mer_mcc_baysian_risk'] + 1e-6) +
    1/(df['hct_term_state_baysian_risk'] + 1e-6) +
    1/(df['ucm_avs_resp_baysian_risk'] + 1e-6) +
    1/(df['hct_term_country_code_baysian_risk'] + 1e-6)
)

# Risk-weighted transaction hour
df['hour_weighted_risk'] = (
    df['trxn_hour'] * df['hct_mer_mcc_baysian_risk']
)

# Weighted average of geo and merchant risks
df['geo_merchant_weighted_risk'] = (
    0.6 * df['hct_term_state_baysian_risk'] +
    0.4 * df['hct_mer_mcc_baysian_risk']
)

# Ratio of high-risk features
df['high_risk_ratio'] = (
    df['hct_mer_mcc_baysian_risk'] /
    (df['hct_term_country_code_baysian_risk'] + 1e-6)
)







import numpy as np
import pandas as pd
from sklearn.decomposition import PCA
from scipy.stats import entropy
from sklearn.ensemble import IsolationForest

# Assuming `df` is your DataFrame

# 1. Risk-Weighted Entropy of MCC Utilization
mcc_risk_distribution = df.groupby('mcc')['hct_mer_mcc_baysian_risk'].transform(lambda x: x / x.sum())
df['risk_weighted_mcc_entropy'] = df.groupby('mcc')['tran_util'].transform(lambda x: entropy(x * mcc_risk_distribution))

# 2. Temporal Risk Aggregation with Decay Factor
def apply_decay(series, decay=0.95):
    return series[::-1].expanding().apply(lambda x: np.sum(x * (decay ** np.arange(len(x))[::-1])))[::-1]
df['temporal_risk_decay'] = apply_decay(df['hct_mer_mcc_baysian_risk'])

# 3. Principal Component Analysis (PCA) on Risk Variables
risk_features = ['hct_mer_mcc_baysian_risk', 'hct_term_state_baysian_risk', 'ucm_avs_resp_baysian_risk', 'hct_term_country_code_baysian_risk']
pca = PCA(n_components=2)
df[['risk_pca_1', 'risk_pca_2']] = pca.fit_transform(df[risk_features])

# 4. Interaction Between High-Risk MCCs and Zero Authorization Mismatches
df['mcc_zero_auth_interaction'] = df['hct_mer_mcc_baysian_risk'] * df['cnt_za_mis_24hr'] * (df['tran_util'] > 500)

# 5. MCC-Level Anomaly Scores Using Isolation Forest
isolation_forest = IsolationForest(contamination=0.01, random_state=42)
df['mcc_anomaly_score'] = isolation_forest.fit_predict(df[risk_features])

# 6. Risk-Weighted MCC Momentum
def calculate_momentum(series):
    return series.diff().fillna(0)
df['risk_mcc_momentum'] = df.groupby('mcc')['hct_mer_mcc_baysian_risk'].transform(calculate_momentum)

# 7. Multi-Level Risk Amplification
geo_risk_amplification = df['hct_term_country_code_baysian_risk'] * df['hct_term_state_baysian_risk']
df['multi_level_risk'] = geo_risk_amplification * df['hct_mer_mcc_baysian_risk']

# 8. Risk Amplification with Temporal and Geographical Features
df['temporal_geo_risk_amplification'] = df['hct_mer_mcc_baysian_risk'] * (df['trxn_hour'] + df['trxn_wkend_ind']) * df['hct_term_country_code_baysian_risk']

# 9. Advanced Rolling Risk Trends
df['rolling_risk_trend'] = df.groupby('mcc')['hct_mer_mcc_baysian_risk'].transform(lambda x: x.rolling(14).std())

# 10. Dynamic Risk Scoring Based on Peer Groups
peer_group_mean = df.groupby('mcc')['hct_mer_mcc_baysian_risk'].transform('mean')
df['dynamic_peer_risk'] = df['hct_mer_mcc_baysian_risk'] / peer_group_mean

# 11. MCC Risk Propagation Across Transactions
def propagate_risk(series, alpha=0.8):
    propagated = [series.iloc[0]]
    for val in series.iloc[1:]:
        propagated.append(alpha * propagated[-1] + (1 - alpha) * val)
    return pd.Series(propagated)
df['mcc_risk_propagation'] = df.groupby('mcc')['hct_mer_mcc_baysian_risk'].transform(propagate_risk)

# 12. High-Risk Transaction Clustering (Density-Based)
from sklearn.cluster import DBSCAN
clustering_features = ['hct_mer_mcc_baysian_risk', 'tran_util', 'time_since_last_trxn']
dbscan = DBSCAN(eps=0.5, min_samples=5)
df['high_risk_cluster'] = dbscan.fit_predict(df[clustering_features])

# 13. Weighted Risk Trend Deviation
def trend_deviation(series):
    mean_trend = series.expanding().mean()
    return ((series - mean_trend) ** 2).sum()
df['weighted_trend_deviation'] = df.groupby('mcc')['hct_mer_mcc_baysian_risk'].transform(trend_deviation)

# 14. Nonlinear Risk Interactions Using Polynomial Features
from sklearn.preprocessing import PolynomialFeatures
poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)
nonlinear_features = poly.fit_transform(df[risk_features])
poly_columns = [f'poly_feature_{i}' for i in range(nonlinear_features.shape[1])]
df[poly_columns] = nonlinear_features

# 15. Sequential MCC Risk Impact
def sequential_impact(series):
    return series.expanding().apply(lambda x: np.prod(x))
df['sequential_risk_impact'] = df.groupby('mcc')['hct_mer_mcc_baysian_risk'].transform(sequential_impact)

print("Advanced features successfully created!")












import numpy as np
import pandas as pd

# Assuming `df` is your DataFrame

# 1. Risk-Weighted Aggregated Spend by MCC Type
df['risk_weighted_mcc_spend'] = df.groupby('mcc')['tran_util'].transform(lambda x: (x * df['hct_mer_mcc_baysian_risk']).sum())

# 2. Risk-Weighted MCC Volatility
df['risk_weighted_mcc_volatility'] = df.groupby('mcc')['tran_util'].transform(lambda x: (x * df['hct_mer_mcc_baysian_risk']).var())

# 3. Risk Interaction with Temporal Trends
df['risk_temporal_interaction'] = df['hct_mer_mcc_baysian_risk'] * (df['trxn_hour'] + df['trxn_wkend_ind'])

# 4. Cumulative MCC Risk Impact
df['cumulative_mcc_risk'] = df.groupby('mcc')['hct_mer_mcc_baysian_risk'].transform(lambda x: x.cumsum())

# 5. High-Risk MCC Clusters
from sklearn.cluster import KMeans
features = ['hct_mer_mcc_baysian_risk', 'hct_term_state_baysian_risk', 'tran_util']
kmeans = KMeans(n_clusters=5, random_state=42).fit(df[features])
df['high_risk_mcc_cluster'] = kmeans.labels_

# 6. Risk-Weighted Peer Comparison
df['peer_comparison'] = df['tran_util'] / df.groupby('mcc')['tran_util'].transform('mean') * df['hct_mer_mcc_baysian_risk']

# 7. Risk Score Trends for MCCs
df['rolling_mcc_risk_7d'] = df.groupby('mcc')['hct_mer_mcc_baysian_risk'].transform(lambda x: x.rolling(7).mean())
df['rolling_mcc_risk_30d'] = df.groupby('mcc')['hct_mer_mcc_baysian_risk'].transform(lambda x: x.rolling(30).mean())

# 8. Risk Amplification with Zero Auth Mismatches
df['risk_amplification_zero_auth'] = df['hct_mer_mcc_baysian_risk'] * df['cnt_za_mis_24hr']

# 9. Temporal MCC Risk Decay
df['temporal_mcc_risk_decay'] = df.groupby('mcc')['hct_mer_mcc_baysian_risk'].transform(lambda x: x[::-1].expanding().mean()[::-1])

# 10. Risk-Contributing MCC Groups
mcc_groups = {'retail': [1, 2, 3], 'hospitality': [4, 5, 6]}  # Example groups
def get_group_risk(row):
    for group, mccs in mcc_groups.items():
        if row['mcc'] in mccs:
            return df[df['mcc'].isin(mccs)]['hct_mer_mcc_baysian_risk'].sum()
    return 0
df['group_risk_contribution'] = df.apply(get_group_risk, axis=1)

# 11. Risk-Weighed Anomalies in Amount Distribution
mcc_median = df.groupby('mcc')['tran_util'].transform('median')
df['risk_weighted_anomaly'] = ((df['tran_util'] - mcc_median).abs() / mcc_median) * df['hct_mer_mcc_baysian_risk']

# 12. Normalized Risk Ratios for MCCs
mcc_mean_risk = df.groupby('mcc')['hct_mer_mcc_baysian_risk'].transform('mean')
df['normalized_risk_ratio'] = df['hct_mer_mcc_baysian_risk'] / mcc_mean_risk

# 13. Multi-Risk Composite Scores
df['multi_risk_composite'] = (
    0.4 * df['hct_mer_mcc_baysian_risk'] +
    0.3 * df['hct_term_state_baysian_risk'] +
    0.2 * df['ucm_avs_resp_baysian_risk'] +
    0.1 * df['hct_term_country_code_baysian_risk']
)

# 14. High-Risk MCC Burst Patterns
def detect_burst(series):
    return ((series.diff() < pd.Timedelta('2 hours')) & (series > series.mean())).sum()
df['mcc_burst_pattern'] = df.groupby('mcc')['tran_util'].transform(detect_burst)

# 15. Risk Amplification by Geography
df['risk_geo_amplification'] = df['hct_mer_mcc_baysian_risk'] * df['hct_term_country_code']

print("Features successfully created!")










# Code to generate all 20 features for fraud detection

# Risk-weighted transaction hour feature
df['risk_trxn_hour'] = df['hct_mer_mcc_baysian_risk'] * df['trxn_hour']

# Risk-weighted weekend feature
df['risk_trxn_weekend'] = df['hct_term_state_baysian_risk'] * df['trxn_wkend_ind']

# Risk-weighted foreign indicator feature
df['risk_foreign_ind'] = df['hct_term_country_code_baysian_risk'] * df['foreign_ind']

# Risk-weighted transaction utility feature
df['risk_tran_util'] = df['hct_mer_mcc_baysian_risk'] * df['tran_util']

# Risk-weighted gift card indicator feature
df['risk_gift_card'] = df['hct_mer_mcc_baysian_risk'] * df['gift_card_ind']

# Risk-weighted even-dollar indicator feature
df['risk_even_dollar'] = df['hct_term_country_code_baysian_risk'] * df['even_dollar_ind']

# Risk-weighted customer present feature
df['risk_cust_present'] = df['hct_term_state_baysian_risk'] * df['cust_present_ind']

# Risk-weighted CVV mismatch feature
df['risk_cvv_mismatch'] = df['ucm_avs_resp_baysian_risk'] * df['cvv2_mismatch_ind']

# Risk-weighted AVS complete mismatch feature
df['risk_avs_complete_mismatch'] = df['ucm_avs_resp_baysian_risk'] * df['avs_complete_mismatch_ind']

# Risk-weighted time since last transaction feature
df['risk_time_since_last_transaction'] = df['hct_term_state_baysian_risk'] / (df['time_since_last_trxn'] + 1)

# Risk-weighted same amount bursts feature
df['risk_same_amount_bursts'] = df['hct_mer_mcc_baysian_risk'] * df['cnt_same_amt_2hr']

# Risk-weighted combined CVV and AVS mismatch feature
df['risk_cvv_avs_combo_mismatch'] = (
    df['hct_term_country_code_baysian_risk'] * 
    (df['cvv2_mismatch_ind'] + df['avs_mismatch_ind'])
)

# Risk-weighted zero authorization feature
df['risk_zero_auth_anomalies'] = df['hct_mer_mcc_baysian_risk'] * df['zero_auth_ind']

# Risk-weighted geographic repeats feature
df['risk_high_risk_geographic_repeats'] = df['hct_term_state_baysian_risk'] * df['cnt_same_pos_amt_24hr']

# Risk-weighted cross-country fraud feature
df['risk_cross_country_fraud'] = (
    df['hct_term_country_code_baysian_risk'] + df['hct_term_state_baysian_risk']
) * df['foreign_ind']

# Risk-weighted authorization behavior feature
df['risk_auth_behavior_anomalies'] = (
    (df['auth_ecom_ind'] + df['auth_ecom_1'] + df['auth_ecom_2']) * 
    (df['hct_mer_mcc_baysian_risk'] + df['hct_term_state_baysian_risk'])
)

# Risk-weighted recurrence behavior feature
df['risk_recurrence_behavior'] = (
    df['hct_mer_mcc_baysian_risk'] * 
    (df['token_recur_bill_ind'] + df['recur_bill_ind'])
)

# Risk-weighted total amount in 24 hours feature
df['risk_total_amount_24hr'] = df['hct_mer_mcc_baysian_risk'] * df['tot_amt_cnt_24hr']

# Risk-weighted swipe activity feature
df['risk_frequent_swipe_activities'] = df['hct_term_state_baysian_risk'] * df['avg_swipe_amt_24hr']

# Risk-weighted clustered behavior feature
df['risk_clustered_behavior'] = (
    df['hct_mer_mcc_baysian_risk'] * 
    (df['cnt_same_amt_6hr'] + df['cnt_same_pos_24hr'])
)
